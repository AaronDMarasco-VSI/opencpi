To/from simulator:

The width issus and how it applies to SW:

The SOM/EOM is derived from the header so that is irrelevant.

The length is defined in 64 bit quanitities so length is clear from the data.

The issue is how this works with various widths.

There are two operations between streams of DWs and beats of the SDP.

One side is streaming DW and the other is SDP beats.

So streamDW-to-SDP is:

States are:
1. Between messages - ready for first/short hdr
2. Between first and second header words.
3. Doing payload, counting payload DWs.

State is # of header DWs.

Issue:  What about segments of messages?
SDP is NOT about OpenCPI messages, it is about RDMA.
If we were going directly to streaming, we would have to allow
for fragments/segments.
If we were going to memory, we don't have that problem.
DW are always naturally aligned.
Header words are in the "high" positions to allow smaller
transfers to be aligned.

FIFO flow:

in the valid/take model the consumer does not offer "ready", which is crazy, since we can't
conditionalize/test whether we need buffering, which means unnecessary upstream buffering.
Or "pushd the need for buffering downstream when possible.

Limits using short headers:

7  reads outstanding
7  DPs
31 workers
128KB per worker
4MB per DP


----------

Addressing from outside:

For PCI we have two BARs and route control to one and non-control to the other.

We could use subdevices too.

For SDP we have nodes, where the zero node is control.

For PCI2SDP we could use PCI functions to map to nodes,
or BARs, or whatever.  Current PCI address space use is smaller with the BARS,
since the address space of the DP "nodes" is smaller.  but the SDP doesn't need to deal
with that...
Ultimately we could make the various header fields some sort of discoverable thing...

ZERO Length:  PCI says this is meaningful, and this is encoded in the byte enables..
For the lead/trail scheme this is equivalent to a lead+trail == 4

Multiplexing:
  outbound requests
  down stream requests (like from PCI) as assigned by the mux function.
  consider peer-to-peer too. 
  On PCI the xids are unique to the requester regardless of the target.
  for an SDP, the interconnect adapter thus takes in a truly unique "requester" identity,
  that for PCI is 16 bits of "requester id" and 5 or 8 bits of "xid" for a total of
  24 bits for the unique xid across PCI.  This is all about completion routing for READ
  transactions.

  For SDP the IC bridge must capture the global XID, and map it into a local
  XID within the node.  Then when the response comes back, the local XID
  allows the bridge to match it up.  Of course responses have plenty of spare bits
  so they are used to convey who it is from (which node). 

  This the responding node plus the request's XID are looked up in the bridges table
  to get the PCI requester ID and original tag.

  E.g. if we have 8 nodes, and 8 XIDs per node, a response will have 6 bits of
  "request identity" to look up the actual PCI requester and tag (24 bits).
  This essentially reduces the header size by 21 bits.

  For outbound requests to PCI, we are essentially targeting a destination node,
  and thus the node number is concatentated with the address to form a global address,
  that should be enough for "address-based" routing.  It is an error if there are not
  enough address bits.

  For outbound responses, the bridge must use the node number to find not only
  the global transation ID (with requester), but also the local "completer" id.
  Finally, to implement partial completions there are more bits:
  -- The address is the address within the request of this completion
  -- The count of the data indicates the amount of data sent.
  Since the bridge is maintaining knowledge of the transaction, it can
  generate the funky "byte count remaining" required by PCI.

  This implies that it is the responsibility of SDP nodes to perform fragmentation,
  NOT the bridge, which is somewhat counterintuitive, but DPs need to be able to do
  fragmentation for the proper functioning of the SDP in any case.

  To summarize for read responses:
  1. The node field is the responding node, and is thus an echo from the request
  2. The XID field is also an echo of the request.
  3. The address field is the data WITHIN the request
  4. How to compute the "bytes left including this one" field?
     -- If we burden the nodes, it means the bridge never needs to update the table.
     -- The minimum info necessary is the remaining count.
     -- Otherwise the responses will cause a BRAM write with the updated transation,
        which might be nice...
     -- the "end of response" is essentially remaining byte count == this byte count.
  

  
  
===========

plumbing has width parameters which can be prebuilt nicely for each size.


sdp_node has position.

==============

The "outer" issue may not be the write qualifier to suppress the generation
of the VHDL non-record interface.
pf workers need this too.
Perhaps back to HdlDevice and HdlPlatform etc...

emitsignals(lang, record, package, worker)  rec   pkg   inwkr
the worker entity:                         (true,  true,  true)
the record interface _rv in the defs file: (true,  true,  false) and config & container entity
the interface for verilog in the defs file:(false, true,  false)*
the actual wrapper in the impl file:       (false, false, false)*

=========

When building a platform worker that incorporates another (device) worker,
how does it include the right library since it is not an assembly...

Put another way, the container incorporates an OCDP worker which is build in hdl/devices.
But it finds the component libraries because it is an assembly.

Iin a platform worker, it does not find the component libraries....

if sdp_node is just in the sdp library:
- no prebuilding with different parameters
- always synthesized by using parameters.
- no change to tune and optimize
- no need to have platform workers incorporate real workers...

==========

What pipelining is needed and valuable in the node?

How much latency is imposed per node?

Let's not build in pipelining.
Let's keep all endpoints simple.
The node is a good place to introduce pipelining, possibly customized.
Perhaps we pipeline on the downstream leg which allows the first one to be 
flow through?

For the interconnect, it might know its own buffering requirements anyway.

Perhaps we just have sdp buffers we can insert...

------------

sdp_send:

Is a scatter-capable transfer
Fed by someone who is already acting as a bridge, but feeding
us messages that need further fanout. i.e. we are one fanout engine from
one on-chip source to one interconnect.

Let's do it passive first, which will be simpler.
1. Sufficient for slow sim DP
2. Debug broken passive mode
3. Requires non-master new SDP.

So, other than programmable and debug registers, there must be a doorbell array and a
"navailable" array.
The writable doorbells are used to free up slots and thus increment the number of
slots that are empty.
The readable locations are the number of full buffers.
So the writable doorbells are not storage at all, but we can shadow the addresses anyway.
So the buffer states are:
empty - available for local side
being filled - local side is busily filling
full - available to be read by other side
so local interface is:
next2fill (or being filled)
a bit that says being filled.
Programmables that can are static, in properties
- number of remotes
- number of buffers
- buffer size
- number available for remote
- number available for local
- current being filled
- current being transmitted
- current remote being sent to
internal: being filled or next to fill



Discovery of sims and multiple access.

There are two ways to talk to a sim.  UDP and SDP over TCP.

There are four states to a sim:

1. Discoverable, with no client active, and no simulation running
2. Client active and loading sim (starting a simulation)
3. Client active, with simulation running and SDP socket open to client.
4. Simulation running with no client active.

There are 4 sockets:
 m_xferSrvr - server is waiting for client to connect for transferring executable
 m_xferSckt - server receives executable over this
 m_sdpSrvr  - Server is waiting for client to connect to establish SDP
 m_sdpSckt  - server received SDP traffic.

There is ann indication of a subprocess:
 s_pid - simulation subprocess

Thus a single process CANNOT host multiple simulations yet.



-------------

On to the fanout aspects:

Have the taken flags per buffer, and have them count up to nremotes,
before advancing.
Do we need different addresses?  Maybe not.

We have nremotes.
For each we need to program the set:
base/pitch
meta/pitch
flag/pitch
nbuffers/free
Worst case is 16 DW each.
Then there is the dynamic ones.
So BRAM is it, which implies pipelining...

Out outer loop can skip ones that are congested.
So outer loop is just trying each one.
Need segmentation too.
Write in SW...
